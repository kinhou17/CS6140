Problem01: Using the Abalone Data Set, build a system to predict the age of an abalone from physical features.  Try both a multi-class classifier and a regressor.  For the regressor, you decide what a correct output is (within x from truth).   The description of the data is here.  The dataset is here.


        This problem began with importing the data and labelling the columns. Then I checked for nulls, of which there were none. Then, I checked for non-numerical values, which there was one (Sex). I turned this into three binary categories so that it would be numerically represented.  After plotting and visualizing the data I could see that all the categories, other than Sex, tended to follow normal distribution curves. 
        After splitting the data, I tried multiple multiclass classifiers. I tested SGD classifier, one vs one classifier, and random forest classifier against the training data. The random forest classifier worked the best against the training data with the highest accuracy scores. Plotting the random forest model in a confusion matrix shows a clear diagonal line in the graph, which is what we are looking for, meaning the majority of the data was classified correctly. I also tried linear regression, which produced even better results. In the end, when running both random forest and linear regression on the test data, I found linear regression to be the most accurate, with a root mean square error (RMSE) of 2.17, compared to random forestâ€™s RMSE of 2.52.
        After all this, I tried to improve the model using gradient boosting, using a decision tree regressor as shown in the book. My hope was that this model would be more accurate, as it sequentially adds predictors while correcting its predecessor. Normal gradient boosting resulted in a RMSE of 2.12, which is an improvement over the random forest model. I also tried gradient boosting with early stopping (a form of regularization) that stops the model after the validation error does not improve for a certain number of iterations in a row. I tried different values for this number, which default was set to 5, but found normal gradient boosting to produce around the same results. With an RMSE of 2.12, this seems to be the most accurate model and considering the range of ages(rings) we are trying to predict (1-29) and the data sample size (~4000), I think this result is quite reasonable of being able to predict an abalones age plus or minus about 2 years.
        I wanted to try another form of regularization at the end, so I tested out Ridge regression. Ridge regression is a regularized version of linear regression, and since linear regression had performed well I thought this would be a good model to try as well. With this model, I achieved a RMSE of 2.16, which is very respectable but still not quite as good as gradient boosting.